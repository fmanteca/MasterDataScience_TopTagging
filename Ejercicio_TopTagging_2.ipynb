{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcM78kST5xyi"
   },
   "source": [
    "# Top Tagging exercise 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YuVRHE055xyk"
   },
   "source": [
    "## Another approach to the physics problem: jet images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08BibaDm5xyl"
   },
   "source": [
    "In the previous notebook (TopTagging_1), we have described a very easy approach to tackle a high energy physics problem. In this exercise, we will learn a different approach, that represents the state-of-the-art of a more general problem, that is, image recognition.\n",
    "\n",
    "We have described what is a jet and how we can reconstruct it, by clustering together the particles produced in the hadronization process of a quark (or a gluon). Let's think about the CMS detector: its shape is basically that of a cylinder. The cylindrical surface of the detector can be unrolled along the radial and the longitudinal coordinates, This surface, that will be a rectangle, can then be divided into \"pixels\". The particle energy deposits can be converted into \"colour intensities\" within each pixel. The more dense and the more energetic the particles, the more color density in one particular pixel.\n",
    "\n",
    "The idea behind jet images is a specific application of the previously described approach: the energy deposits of the jets constituents are transformed into \"intensities\" of a 2D black and white image. Image recognition algorithms can be therefore applied to a high energy physics problem.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/images_jets.png\" width=\"800\" >\n",
    "\n",
    "https://arxiv.org/abs/1612.01551\n",
    "\n",
    "The images that we are going to use require some pre-processing, such as being properly normalized and oriented. The details are included in the following papers:\n",
    "* https://arxiv.org/abs/1701.08784\n",
    "* https://arxiv.org/abs/1803.00107\n",
    "* https://arxiv.org/abs/1407.5675\n",
    "* https://arxiv.org/abs/1511.05190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4YGcnzN5xyn"
   },
   "source": [
    "## Image recognition techniques: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PedZ2YNE5xyo"
   },
   "source": [
    "Convolutional neural network (**CNN**) is a class of deep neural networks applied to classify images. A simple CNN is a sequence of layers, and every layer transforms one volume of activations to another through a differentiable function. Two additional types of layers will be used in this exercise, together with the Fully Connected Layer (as in the previous exercise): **Convolutional Layer** and **Pooling Layer**.\n",
    "\n",
    "### Convolutional layer\n",
    "\n",
    "Every image can be considered as a matrix of pixel values, describing the color intensity per pixel. (https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "Let's consider a 5x5 image whose pixel values are only 0 and 1:\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/matrix1.png\" width=\"150\" >\n",
    "\n",
    "Let's consider another 3x3 matrix, that we will call `filter`:\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/matrix2.png\" width=\"100\" >\n",
    "\n",
    "The Convolution of the 5x5 image and the 3x3 filter computation can be visualized as follows:\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/matrix3.gif\" width=\"300\" >\n",
    "\n",
    "We slide the orange matrix over our original image (green) by 1 pixel (also called `stride`). For every position of the filter, we compute element wise multiplication (between the two matrices), and we add the multiplication outputs to get the final integer which forms a single element of the output matrix (pink). Note that the 3×3 matrix \"sees\" only a part of the input image in each stride.\n",
    "\n",
    "Once applied to general images, these filters can develop the ability to detect edges, sharpen or blur an image, or even to detect particular objects within a full picture.\n",
    "\n",
    "A CNN learns the values of these filters (that are our network **weights**) during the training process. The more filters we have, the more image features get extracted and the better our network becomes at recognizing patterns in unseen images.\n",
    "\n",
    "The size of the Convolved output (or `feature map`) is controlled by three parameters that we must decide before the convolution step is performed:\n",
    "\n",
    "* `Depth`: depth corresponds to the number of filters we use for the convolution operation. We can think of these three feature maps as stacked 2D matrices.\n",
    "* `Stride`: stride is the number of pixels by which we slide our filter matrix over the input matrix. When the stride is 1, then we move the filters one pixel at a time. When the stride is 2, then the filters jump 2 pixels at a time as we slide them around. Having a larger stride will produce smaller feature maps.\n",
    "* `Zero-padding`: sometimes, it is convenient to pad the input matrix with zeros around the border, so that we can apply the filter to bordering elements of our input image matrix. A nice feature of zero padding is that it allows us to control the size of the feature maps.\n",
    "\n",
    "### Pooling layer\n",
    "\n",
    "`Pooling` (or downsampling) reduces the dimensionality of each feature map, but it retains the most important information. Pooling can be of different types: Max, Average, Sum.\n",
    "\n",
    "In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the feature map within that window. Instead of taking the largest element we could also take the average (average pooling) or sum of all elements in that window. Max Pooling has been shown to work better.\n",
    "\n",
    "One can set the pooling stride as well. Pooling makes the input representations (feature dimension) smaller and more manageable, and it reduces the number of parameters and computations in the network, or, in other words, it controls overfitting.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/pooling.png\" width=\"500\" >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hy9x7JgH5xyq"
   },
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itjM2dj75xyr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "!/opt/conda/bin/python3.11 -m pip install --upgrade tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sE81CBM15xyv"
   },
   "source": [
    "First, let's have a look at the shape of the input training data. They are stored as pandas **`DataFrame`** HDF5 data (see `pandas` notebook), in the format of `PyTables`. Please note, this time we are using another input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZkg75Ut5xyx"
   },
   "outputs": [],
   "source": [
    "#This is selecting the full dataset, it will take a while. Comment afterwards!\n",
    "store_train_full = pd.HDFStore(\"/home/jovyan/share/TopTaggingData/train_img.h5\", mode='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n50mbxa35xzB"
   },
   "source": [
    "This time, we have converted the information of the energy of the jet constituents into an image. Each image has 40x40 pixels = 1600 pixels. Each column represents the *colour* intensity in each pixel.\n",
    "\n",
    "Let's define a function, `to_image`, that rewrites these columns as a 40x40 numpy matrix, with one additional index, that represents the colours of the image (see later boxes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6z3Hovy5xzC"
   },
   "outputs": [],
   "source": [
    "# 1 image has 40x40 pixels = 1600 pixels\n",
    "pixels = [\"img_{0}\".format(i) for i in range(1600)]\n",
    "\n",
    "def to_image(df):\n",
    "    return  np.expand_dims(np.expand_dims(df[pixels], axis=-1).reshape(-1,40,40), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACPsJF8g5xzG"
   },
   "source": [
    "Let's now read only the first 10 k events that will be used for training (you can increase this number later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkiZDAVT5xzH"
   },
   "outputs": [],
   "source": [
    "# Read the first 10k events\n",
    "df_train = store_train_full.select(\"table\", stop=10000)\n",
    "images_train = to_image(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFXlj4CQ5xzK"
   },
   "source": [
    "One useful hint: keep the last events of the training set as test samples to evaluate performances. In order to do so, you can use **`stop`** and **`start`** options in **`select`**. Let's prepare a test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxZ9GZJ55xzM"
   },
   "outputs": [],
   "source": [
    "# Keep the last 2 k events as test sample\n",
    "df_test = store_train_full.select(\"table\",start= 1200000 - 2000)\n",
    "images_test = to_image(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxSZhCmV5xzP"
   },
   "source": [
    "Let's have a look at the shape of the input training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXstHFbm5xzQ"
   },
   "outputs": [],
   "source": [
    "images_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWblbsGV5xzU"
   },
   "source": [
    "As stated before, this time we have a more complex input shape, a multidimensional matrix with four indices:\n",
    "\n",
    "* The first index is the number of training events\n",
    "* The second index labels the pixels of the images (40) along the horizontal axis\n",
    "* The third index labels the pixels of the images (40) along the vertical axis\n",
    "* The fourth index labels the colours of the images\n",
    "\n",
    "In modern image recognition architectures, it is very common to decompose an image into three colour layers: red, green and blue. Our jet images have only one colour (they are black and white), since we are converting the momenta of the particles hitting that particular region of the detector as a color intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vvbzMM85xzW"
   },
   "outputs": [],
   "source": [
    "print(images_train[0][20])\n",
    "#One non empty column of pixels along the 20th horizontal pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeAZCWbV5xzc"
   },
   "source": [
    "Let's plot the actual jet images. Let's perform an average of all our training sample. This is because, one jet image alone is very sparse (only few non-empty pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9DjFEAN5xzd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Save all signal events in one DataFrame\n",
    "df_signal = df_train[df_train[\"is_signal_new\"]==1]\n",
    "# Save all background events in one DataFrame\n",
    "df_background = df_train[df_train[\"is_signal_new\"]==0]\n",
    "\n",
    "#Perform an average of signal and background DataFrame\n",
    "df_signal_sum = df_signal.sum(axis = 0, skipna = True)/len(df_signal.index)\n",
    "df_background_sum = df_background.sum(axis = 0, skipna = True)/len(df_background.index)\n",
    "\n",
    "#Define an easier function to convert the averaged images into 40x40 matrices\n",
    "def to_image_plot(df):\n",
    "    return  df[pixels].values.reshape(40,40)\n",
    "\n",
    "image_signal = to_image_plot(df_signal_sum)\n",
    "image_background = to_image_plot(df_background_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_signal.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WFUjbRDU5xzg"
   },
   "source": [
    "Let's plot signal and background images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ay3rsy9U5xzi"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2,figsize=(15,15))\n",
    "#plt.subplots_adjust(hspace=0.3)\n",
    "plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "\n",
    "axs[0].imshow(image_signal,cmap=\"Reds\")\n",
    "axs[0].set_title('Signal')\n",
    "\n",
    "axs[1].imshow(image_background,cmap=\"Blues\")\n",
    "axs[1].set_title('Background')\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='$\\eta$', ylabel='$\\phi$')\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wuO4J9O5xzn"
   },
   "source": [
    "As expected from our physics problem, jets reconstructing top quark decays show a three-pronged structure, whilst background events initiated by strong interaction show a one-pronged structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmZOGr735xzo"
   },
   "source": [
    "## Building the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rg3zD3Z5xzp"
   },
   "source": [
    "Let's start by defining the last layer, that, as in the first part of the exercise, should have a softmax activation function. A keras **`Dense`** layer accepts only vectors as input, and not matrices. The **`Flatten`** function transforms our 40x40 matrix into one single vector.\n",
    "\n",
    "https://keras.io/layers/core/#flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_tiA9xJ5xzr"
   },
   "outputs": [],
   "source": [
    "# Define the network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(40,40,1)))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIZN9n_v5xzz"
   },
   "source": [
    "As discussed during the first part of this lab, a very natural choice for loss function and optimizer are, respectively, **`categorical_crossentropy`** and **`adam`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDQPd9Ph5xz3"
   },
   "outputs": [],
   "source": [
    "# Compile the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S00zFtA65xz7"
   },
   "source": [
    "As discussed in the introductory tutorial, we can control **overfitting** with a validation sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcfVoMeo5x0B"
   },
   "outputs": [],
   "source": [
    "# Load validation sample, only the first 2k events\n",
    "# They are 20k in total. Use a larger number afterwards\n",
    "store_val = pd.HDFStore(\"/home/jovyan/share/TopTaggingData/val_img.h5\", mode='r')\n",
    "df_val = store_val.select(\"table\",stop=2000)\n",
    "images_val = to_image(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elzrQDW45x0F"
   },
   "source": [
    "We are now ready to train our model. Let's first try with 5 epochs. The `is_signal_new` column contains the truth information, i.e. being signal (`is_signal_new=1`) or background (`is_signal_new=0`). That's what the network is supposed to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h66Lcw9H5x0G"
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "histObj = model.fit(images_train, keras.utils.to_categorical(df_train[\"is_signal_new\"]), epochs=5, validation_data=(images_val,keras.utils.to_categorical(df_val[\"is_signal_new\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMjDsEbN5x0L"
   },
   "source": [
    "Let's now plot the learning curves for training and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVIm9kFL5x0N"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def plotLearningCurves(*histObjs):\n",
    "    \"\"\"This function processes all histories given in the tuple.\n",
    "    Left losses, right accuracies\n",
    "    \"\"\"\n",
    "    # too many plots\n",
    "    if len(histObjs)>10: \n",
    "        print('Too many objects!')\n",
    "        return\n",
    "    # missing names\n",
    "    for histObj in histObjs:\n",
    "        if not hasattr(histObj, 'name'): histObj.name='?'\n",
    "    names=[]\n",
    "    # loss plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "    plt.subplot(1,2,1)\n",
    "    # loop through arguments\n",
    "    for histObj in histObjs:\n",
    "        plt.plot(histObj.history['loss'])\n",
    "        names.append('train '+histObj.name)\n",
    "        plt.plot(histObj.history['val_loss'])\n",
    "        names.append('validation '+histObj.name)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper right')\n",
    "    \n",
    "\n",
    "    #accuracy plot\n",
    "    plt.subplot(1,2,2)\n",
    "    for histObj in histObjs:\n",
    "        plt.plot(histObj.history['accuracy'])\n",
    "        plt.plot(histObj.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    #plt.ylim(0.5,1)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # min, max for loss and acc\n",
    "    for histObj in histObjs:\n",
    "        h=histObj.history\n",
    "        maxIdxTrain = np.argmax(h['accuracy'])\n",
    "        maxIdxTest  = np.argmax(h['val_accuracy'])\n",
    "        minIdxTrain = np.argmin(h['loss'])\n",
    "        minIdxTest  = np.argmin(h['val_loss'])\n",
    "        \n",
    "        strg='\\tTrain: Min loss {:6.3f} at {:3d} --- Max acc {:6.3f} at {:3d} | '+histObj.name\n",
    "        print(strg.format(h['loss'][minIdxTrain],minIdxTrain,h['accuracy'][maxIdxTrain],maxIdxTrain))\n",
    "        strg='\\tValidation : Min loss {:6.3f} at {:3d} --- Max acc {:6.3f} at {:3d} | '+histObj.name\n",
    "        print(strg.format(h['val_loss'][minIdxTest],minIdxTest,h['val_accuracy'][maxIdxTest],maxIdxTest))\n",
    "        print(len(strg)*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvEclZiS5x0Q"
   },
   "outputs": [],
   "source": [
    "histObj.name='' # name added to legend\n",
    "plotLearningCurves(histObj) # the above defined function to plot learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-nkItaJ5x0V"
   },
   "source": [
    "## Evaluating performances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdLl2VM05x0W"
   },
   "source": [
    "Model evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "ROC curves (wikipedia): https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSXcfcBa5x0W"
   },
   "source": [
    "As already explained in TopTagging_1, let's calculate the AUC and the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDRbVnnz5x0Y"
   },
   "outputs": [],
   "source": [
    "# Needed libraries\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlHffRD15x0b"
   },
   "outputs": [],
   "source": [
    "print(\"Running on test sample. This may take a moment.\")\n",
    "probs = model.predict(images_test)#predict probability over test sample\n",
    "AUC = roc_auc_score(df_test[\"is_signal_new\"], probs[:,1])\n",
    "print(\"Test Area under Curve = {0}\".format(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1zaztFZ5x0f"
   },
   "outputs": [],
   "source": [
    "df_test[\"sigprob\"] = probs[:,1] #save probabilities in df\n",
    "fpr, tpr, _ = roc_curve(df_test[\"is_signal_new\"], df_test[\"sigprob\"]) #extract true positive rate and false positive rate\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label='ROC curve (area = {0:.4f})'.format(AUC))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxoEGvgs5x0j"
   },
   "source": [
    "How can these performances be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcOfKbQl5x0k"
   },
   "source": [
    "### Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wPhHoYy5x0k"
   },
   "source": [
    "Let's try an example with Convolutional Layers, **`Conv2D`**.\n",
    "\n",
    "Keras documentation on convolutional layers: https://keras.io/layers/convolutional/\n",
    "\n",
    "Let's have a look at the important parameters we must set:\n",
    "\n",
    "`keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)`\n",
    "\n",
    "* `filters`: the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "* `kernel_size`: 2 integers specifying the height and width of the 2D convolution window (i.e. the dimension of the filters).\n",
    "* `strides`: 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "* `padding`: \"same\" results in padding the input such that the output has the same length as the original input.\n",
    "* `activation`: activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "* `Input shape`: when using this layer as the first layer in a model, provide the keyword argument `input_shape` (tuple of integers), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures.\n",
    "\n",
    "Our `input_shape` will be: `input_shape=(40, 40, 1)` since we have 40x40 black and white pictures.\n",
    "\n",
    "More advanced options are described in the previous link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3B3huQ55x0l"
   },
   "source": [
    "### MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4g0JZXXe5x0m"
   },
   "source": [
    "Another useful layer to use is **`MaxPooling2D`**.\n",
    "\n",
    "Keras documentation on MaxPooling: https://keras.io/layers/pooling/\n",
    "\n",
    "Let's have a look at the important parameter we must set:\n",
    "\n",
    "`keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "* `pool_size`: 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimensions. If only one integer is specified, the same window length will be used for both dimensions.\n",
    "* `strides`: 2 integers, defining the strides values. If None, it will default to pool_size.\n",
    "* `padding`: one of \"valid\" or \"same\". \"valid\" means \"no padding\". \n",
    "\n",
    "More advanced options are described in the previous link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIKx2NlX5x0n"
   },
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential()\n",
    "\n",
    "model2.add(keras.layers.Conv2D(8,(4,4),padding='same',input_shape=(40,40,1)))\n",
    "#Suggestion: add more than 8 filters! Try for example 24, 32, 64...\n",
    "model2.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(keras.layers.Flatten())\n",
    "#Sugestion: add a Dense layer, with relu activation here! Try 50, 100, 200 nodes...\n",
    "model2.add(keras.layers.Dense(2, activation='softmax'))\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCBsOIE65x0s"
   },
   "source": [
    "Let's compile and fit the model. This time the training will take longer, since the model is more complex and it includes more trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uP7da2r05x0s"
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_ejxswR5x0v"
   },
   "outputs": [],
   "source": [
    "histObj2 = model2.fit(images_train, keras.utils.to_categorical(df_train[\"is_signal_new\"]), epochs=5, validation_data=(images_val,keras.utils.to_categorical(df_val[\"is_signal_new\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPCsKXu95x01"
   },
   "source": [
    "Let's evaluate on the test sample and produce the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxRIOaQf5x02"
   },
   "outputs": [],
   "source": [
    "histObj2.name='' # name added to legend\n",
    "plotLearningCurves(histObj2) # the above defined function to plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw8NyG165x09"
   },
   "outputs": [],
   "source": [
    "print(\"Running on test sample. This may take a moment.\")\n",
    "probs2 = model2.predict(images_test)#predict probability over test sample\n",
    "AUC2 = roc_auc_score(df_test[\"is_signal_new\"], probs2[:,1])\n",
    "print(\"Test Area under Curve = {0}\".format(AUC2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPhKDSyq5x1B"
   },
   "outputs": [],
   "source": [
    "df_test[\"sigprob\"] = probs2[:,1] #save probabilities in df\n",
    "fpr2, tpr2, _ = roc_curve(df_test[\"is_signal_new\"], df_test[\"sigprob\"]) #extract true positive rate and false positive rate\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label='Model 1, ROC curve (area = {0:.4f})'.format(AUC))\n",
    "plt.plot(fpr2, tpr2, color='green', lw=2, label='Model 2, ROC curve (area = {0:.4f})'.format(AUC2))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4YzoB6v5x1F"
   },
   "source": [
    "One can already appreciate a significative improvement. How can we do better?\n",
    "\n",
    "* Adding more data is always a good idea. But pay attention: if you have a complicated architecture and a lot of data, the training process will slow down.\n",
    "* Try to use more convolutional layers.\n",
    "* Try different filter sizes and different numbers of filters on convolutional layers.\n",
    "* Play around with **`Pooling`** (different size).\n",
    "* Add more **`Dense`** layers after the convolutional part.\n",
    "* You can try other ways to prevent overfitting, such as **`Dropout`** or **`early stopping`**? (one **`Dropout`** layer with rate 0.3 can already improve *a lot* the overfitting)\n",
    "\n",
    "About **`Dropout`**: https://keras.io/layers/core/#dropout\n",
    "\n",
    "Example:\n",
    "\n",
    "``model2.add(keras.layers.Dropout(rate=0.3))``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPZSgC8X5x1G"
   },
   "source": [
    "# Challenge submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKknEIjv5x1H"
   },
   "source": [
    "Once you manage to improve the network performances, you can submit your results and participate to our ML challenge. The challenge test sample is available in this workspace, but the truth labels (`is_signal_new`) are removed, so that you can't compute the AUC.\n",
    "\n",
    "* You can participate as a single participant or as a team\n",
    "* The winner is the one scoring the best AUC in the challenge test sample\n",
    "* In the next box, you will find some lines of code for preparing an output zip file, containing your model and the weights you obtained out of your training\n",
    "* Choose a meaningful name for your result zip file (i.e. your name, or your team name, but avoid to submit `results.zip`)\n",
    "* Download the zip file and send it to me :-)\n",
    "* You can submit multiple results, paying attention to name them accordingly (add the version number, such as `v1`, `v34`, etc.)\n",
    "* You can use both TopTagging_1 or TopTagging_2 as a starting point (train over constituents or over images)\n",
    "* We will consider your best result for the final score\n",
    "* The winner will be asked to present his architecture\n",
    "\n",
    "**Have fun!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZrqB_X15x1H"
   },
   "outputs": [],
   "source": [
    "### Evaluate performance on independent sample\n",
    "# DO NOT CHANGE BELOW!f\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "# Prepare input\n",
    "store_test_challenge = pd.HDFStore(\"/home/jovyan/share/TopTaggingData/test_without_truth_img_100k.h5\", mode='r')\n",
    "df_test_challenge = store_test_challenge.select(\"table\")\n",
    "images_challenge = to_image(df_test_challenge)\n",
    "\n",
    "# Run DNN\n",
    "print(\"Running on full test sample. This may take a moment.\")\n",
    "ret = model2.predict(images_challenge)\n",
    "#print(ret[:,1])\n",
    "np.save(\"result.npy\",ret[:,1])\n",
    "\n",
    "#Save in zip:\n",
    "#!zip result.zip result.npy\n",
    "import zipfile\n",
    "zipf = zipfile.ZipFile('result.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipf.write('result.npy')\n",
    "zipf.close()\n",
    "print(\"Done. Click below  to download result\")\n",
    "FileLink('result.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ejercicio_TopTagging_2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
