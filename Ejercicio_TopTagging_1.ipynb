{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWiUfw6CEt79"
   },
   "source": [
    "# Top Tagging exercise 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCawlz6REt8D"
   },
   "source": [
    "## Introduction to the physics problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yEognc6Et8G"
   },
   "source": [
    "### The Standard Model and the top quark\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/SM.png\" width=\"400\" >\n",
    "\n",
    "The **Standard Model** of elementary particles represents our knowledge of the microscopic world. It describes the matter constituents (quarks and leptons) and their interactions (mediated by bosons), that are the electomagnetic, the weak and the strong interactions.\n",
    "\n",
    "Among all these particles, the **top quark** still represents a very peculiar case. It is the heaviest known elementary particle (mass of 172.5 GeV) and it has a very short lifetime ($10^{-25}$ seconds): this means we can only see its decay products. It has been discovered in 1995 by CDF and D0 experiments at Tevatron (Fermilab, Chicago). The top quark is considered a key particle to searches for new physics beyond the Standard Model and to precision measurements.\n",
    "\n",
    "The ideal tool for measuring the top quark properties is a particle collider. The **Large Hadron Collider** (LHC), situated nearby Geneva, between France and Switzerland, is the largest proton-proton collider ever built on Earth. It consists of a 27 km circumference ring, where proton beams are smashed at a centre-of-mass energy of 13 TeV (99.999999% of speed of light). At the LHC, 40 Million collisions / second occurs, providing an enormous amount of data. Thanks to these data, **ATLAS** and **CMS** experiments discovered the missing piece of the Standard Model, the Higgs boson, in 2012.\n",
    "\n",
    "During a collision, the energy is so high that protons are \"broken\" into their fundamental components, i.e. **quarks** and **gluons**, that can interact together, producing particles that we don't observe in our everyday life, such as the top quark. The production of a top quark is, by the way, a relatively \"rare\" phenomenon, since there are other physical processes that occur way more often, such those initiated by strong interaction, producing lighter quarks (such as up, down, strange quarks). In high energy physics, we speak about the **cross-section** of a process. We say that, the top quark production has a smaller cross-section than the production of light quarks.\n",
    "\n",
    "The experimental consequence is that distinguishing the decay products of a top quark from a light quark can be extremely difficult, given that the latter phenomenon has a way larger probability to happen.\n",
    "\n",
    "### Experimental signature of top quark in a particle detector\n",
    "\n",
    "Let's first understand what are the experimental signatures and how our detectors work. This is a sketch of the CMS experiment.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/EPS_CMS_Slice.png\" width=\"500\" >\n",
    "\n",
    "A collider detector is organized in layers: each layer is able to distinguish and measure different particles and their properties. For example, the silicon tracker detects each particle that is charged. The electromagnetic calorimeter detects photons and electrons. The hadronic calorimeter detects hadrons (such as protons and neutrons). The muon chambers detect muons (that have a long lifetime and travel through the inner layers).\n",
    "\n",
    "Our physics problem consists into detecting the so-called \"hadronic decay\" of a top quark. The decay chain is sketched here: the top quark decays into a bottom quark and into a $W$ boson, that in turn decays into light quarks (in the picture, up and down quarks).\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/top.png\" width=\"500\" >\n",
    "\n",
    "Our background is, instead, represented by light quark (or gluons) produced by the strong interaction (in jargon, QCD). Here we have a sketch of one possible background event.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/QCD.png\" width=\"200\" >\n",
    "\n",
    "#### Jets\n",
    "\n",
    "Without going into the theoretical details, the nature of particles experiencing the strong interaction (like quarks) is such that they cannot travel free, but they are forced to be \"confined\" into hadrons. One hadron can be seen as a \"combination\" of quarks. Let's think about the electromagnetic interaction: a positive charge and a negative charge are attracted to each other, and they will tend to form a state that is neutral under the electromagnetic interaction. Analogously, quarks try to combine together, forming a bond state that is neutral under the strong interaction. This process is called **hadronization**, and it has a very important consequence. Quarks won't appear as single isolated particles in a detector, but rather as **jets** of particles.\n",
    "\n",
    "There are many different algorithms that are able to reconstruct quarks (and gluons) as jets (i.e., anti-$k_T$ algorithm https://arxiv.org/abs/0802.1189). They basically loop over the shower of particles produced by the hadronization, trying to cluster them together as one single entity. The algorithms are designed such in a way that the momentum of the clustered jet is proportional to the initial energy of the quark. A sketch giving an intuitive idea of a jet is displayed here (Klaus Rabbertz, KIT):\n",
    "\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/Rabbertz_from_quark_to_rec_jet.png\" width=\"500\" >\n",
    "\n",
    "#### Jets substructure\n",
    "\n",
    "Many physically motivated approaches have been used in the past to distinguish a jet initiated by a top quark from jets due to QCD. One remarkable property is the so-called **jet substructure**. The idea is to try to distinguish how many \"sub-jets\" are included in a jet. Out of our sketches presented before, since the top quark decays into three separated quarks, we would expect it to show a three-pronged sub-structure. QCD, on the other hand, is mainly due to single quark/gluon radiation, hence it shows a one-pronged sub-structure. One largely used approach to study the jet substructure is the so called *n-subjettiness* (https://arxiv.org/abs/1011.2268)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5klHYaK_Et8J"
   },
   "source": [
    "## Binary classification problem: the machine learning formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ13_T2rEt8M"
   },
   "source": [
    "Our problem is then classifying a jet produced by a top quark (that we will call **signal** event) from a light quark jet due to strong interaction (**background** event).\n",
    "\n",
    "In this exercise, a quite large dataset is provided. Each event is a Monte Carlo simulation of a jet candidate, that can be signal or background. An integer (0 or 1) will label each jet as background or signal. For each jet, the 4-momenta of its **constituents** (i.e., the particles that are clustered by the jet algorithm) are stored. One can build neural network architectures that are able to understand the nature of a jet only by looking at its constituents.\n",
    "\n",
    "The very first approach we are going to have in this exercise is trying a classical artificial neural network approach, that is, **Fully Connected Neural Networks**. They are very generic structures, that can be applied in many different classification problems, but that sometimes provides many weights and that can be quite inefficient. We will see another approach in the next tutorial.\n",
    "\n",
    "Some bibliography about Fully Connected neural network approaches for top quark tagging can be found here:\n",
    "* https://arxiv.org/abs/1704.02124\n",
    "* https://arxiv.org/abs/1501.05968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfOOUQC1Et8P"
   },
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZPr4pR7Et8S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from tensorflow import keras\n",
    "import numpy as np\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juAw6KYKEt8e"
   },
   "source": [
    "First, let's have a look at the shape of the input training data. They are stored as `pandas` **`DataFrame`** HDF5 data (see `pandas` notebook), in the format of `PyTables`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6TMWHrHEt8g"
   },
   "outputs": [],
   "source": [
    "!/opt/conda/bin/python3.11 -m pip install --upgrade tables\n",
    "\n",
    "#This is selecting the full dataset, it will take a while. Comment afterwards, no need to re-run this box!\n",
    "store_train_full = pd.HDFStore(\"/home/jovyan/share/TopTaggingData/train.h5\")\n",
    "df_train_full = store_train_full.select(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2UrcyeAEt8l"
   },
   "outputs": [],
   "source": [
    "print(df_train_full.shape)\n",
    "print(len(df_train_full.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZhoZK2vEt8w"
   },
   "source": [
    "We have 1211000 rows, i.e. 1211000 different events, and 806 columns (whose meaning will be explained later).\n",
    "\n",
    "One useful hint: keep the last events of the training set as test samples to evaluate performances. In order to do so, you can use **`stop`** and **`start`** options in **`select`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgEFCrPnEt8y"
   },
   "outputs": [],
   "source": [
    "# Read the first 10k events\n",
    "# They are quite few, to be increased in later steps\n",
    "#store_train = pandas.HDFStore(\"/root/data/train.h5\")\n",
    "df_train = store_train_full.select(\"table\",stop=10000)\n",
    "print(df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi9ia3ucEt86"
   },
   "outputs": [],
   "source": [
    "# Keep the last 2 k events as test sample\n",
    "# Suggested in next steps: take up to 100k events for testing\n",
    "# But remember: do not use the same events for both training and testing!\n",
    "df_test = store_train_full.select(\"table\",start=1211000-2000)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAN8q2bqEt9B"
   },
   "source": [
    "Let's print out the first three rows of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uqhh_vHYEt9D"
   },
   "outputs": [],
   "source": [
    "df_test.iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZpq6YG1Et9K"
   },
   "source": [
    "The first 800 columns contain the 4-momenta, i.e. energy, $p_x$, $p_y$ and $p_z$, of the first 200 jet constituents. If the constituents are less than 200, the values of the 4-momenta are filled with zeros. The `is_signal_new` column contains the truth information, i.e. it tells if that particular event is signal (`is_signal_new`=1) or background (`is_signal_new`=0). We can disregard the columns starting with \"truth\" and \"ttv\".\n",
    "\n",
    "Let's first focus on the first 20 particle constituents and let's define a list of strings that will be very convenient when loading the pandas **`DataFrame`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgRizrwNEt9M"
   },
   "outputs": [],
   "source": [
    "# four-momenta of leading 20 particles\n",
    "cols = [c.format(i) for i in range(20) for c in [\"E_{0}\",  \"PX_{0}\",  \"PY_{0}\",  \"PZ_{0}\"]]\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGRt0_mxEt9S"
   },
   "source": [
    "## Building the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySz8LA1XEt9X"
   },
   "source": [
    "We can now start to define a first architecture. The most simple approach is using fully connected layers (**`Dense`** layers in Keras/Tensorflow), with **`relu`** activation function and a **`softmax`** final layer, since we are affording a binary classification problem.\n",
    "\n",
    "We are considering the 4-momenta of the first 20 jet constituents, i.e. we expect to have a vector of 80 numbers as **`input_shape`**, that should be specified in the first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7JS3CNcEt9Y"
   },
   "outputs": [],
   "source": [
    "# Define the network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, input_shape = (80,), activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsML8l5kEt9f"
   },
   "source": [
    "As discussed during the first part of this lab, a very natural choice for loss function and optimizer are, respectively, **`categorical_crossentropy`** and **`adam`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBfdhg3CEt9g"
   },
   "outputs": [],
   "source": [
    "# Compile the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYeTuRAMEt9n"
   },
   "source": [
    "As discussed in the introductory tutorial, we can control **overfitting** with a validation sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxyZc9jpEt9p"
   },
   "outputs": [],
   "source": [
    "# Load validation sample, only the first 2k events\n",
    "# They are 20k in total. Use a larger number afterwards\n",
    "store_val = pd.HDFStore(\"/home/jovyan/share/TopTaggingData/val.h5\")\n",
    "df_val = store_val.select(\"table\",stop=2000)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ptb7Hs-Et9u"
   },
   "source": [
    "We are now ready to train our model. Let's first try with 10 epochs. The `is_signal_new` information, i.e. being signal (`is_signal_new=1`) or background (`is_signal_new=0`), is what the network is supposed to learn. Let's save the outputs of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dt0SWRrMEt9v"
   },
   "outputs": [],
   "source": [
    "histObj = model.fit(df_train[cols], keras.utils.to_categorical(df_train[\"is_signal_new\"]), epochs=10, validation_data=(df_val[cols], keras.utils.to_categorical(df_val[\"is_signal_new\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeX9_QVgEt90"
   },
   "source": [
    "We can take  plot the learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0PPWidjEt91"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def plotLearningCurves(*histObjs):\n",
    "    \"\"\"This function processes all histories given in the tuple.\n",
    "    Left losses, right accuracies\n",
    "    \"\"\"\n",
    "    # too many plots\n",
    "    if len(histObjs)>10: \n",
    "        print('Too many objects!')\n",
    "        return\n",
    "    # missing names\n",
    "    for histObj in histObjs:\n",
    "        if not hasattr(histObj, 'name'): histObj.name='?'\n",
    "    names=[]\n",
    "    # loss plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "    plt.subplot(1,2,1)\n",
    "    # loop through arguments\n",
    "    for histObj in histObjs:\n",
    "        plt.plot(histObj.history['loss'])\n",
    "        names.append('train '+histObj.name)\n",
    "        plt.plot(histObj.history['val_loss'])\n",
    "        names.append('validation '+histObj.name)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper right')\n",
    "    \n",
    "\n",
    "    #accuracy plot\n",
    "    plt.subplot(1,2,2)\n",
    "    for histObj in histObjs:\n",
    "        plt.plot(histObj.history['accuracy'])\n",
    "        plt.plot(histObj.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    #plt.ylim(0.5,1)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # min, max for loss and acc\n",
    "    for histObj in histObjs:\n",
    "        h=histObj.history\n",
    "        maxIdxTrain = np.argmax(h['accuracy'])\n",
    "        maxIdxTest  = np.argmax(h['val_accuracy'])\n",
    "        minIdxTrain = np.argmin(h['loss'])\n",
    "        minIdxTest  = np.argmin(h['val_loss'])\n",
    "        \n",
    "        strg='\\tTrain: Min loss {:6.3f} at {:3d} --- Max acc {:6.3f} at {:3d} | '+histObj.name\n",
    "        print(strg.format(h['loss'][minIdxTrain],minIdxTrain,h['accuracy'][maxIdxTrain],maxIdxTrain))\n",
    "        strg='\\tValidation : Min loss {:6.3f} at {:3d} --- Max acc {:6.3f} at {:3d} | '+histObj.name\n",
    "        print(strg.format(h['val_loss'][minIdxTest],minIdxTest,h['val_accuracy'][maxIdxTest],maxIdxTest))\n",
    "        print(len(strg)*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebWXCkSWEt96"
   },
   "source": [
    "Let's now plot the learning curves for training and validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbWDJptwEt98"
   },
   "outputs": [],
   "source": [
    "histObj.name='' # name added to legend\n",
    "plotLearningCurves(histObj) # the above defined function to plot learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vco-5tXNEt-F"
   },
   "source": [
    "## Evaluating performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "115fJSsdEt-G"
   },
   "source": [
    "Model evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "ROC curves (wikipedia): https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUUCzRFTEt-I"
   },
   "source": [
    "There are many ways to evaluate the quality of a model’s predictions. In the previous set of exercises, we were evaluating the metrics and losses of the training and validation samples.\n",
    "\n",
    "A largely used evaluation metrics for binary classification tasks is the receiver operating characteristic curve, or **ROC curve**.\n",
    "\n",
    "First, we introduce the terms `positive` and `negative` referring to the classifier’s prediction, and the terms `true` and `false` referring to whether the network prediction corresponds to the observation (the \"truth\" level). In our top-quark tagging exercise, we can think the `negative` outcome as the one labelling background (that, in the last softmax layer of our network would mean a number close to 0), and the `positive` outcome as the one labelling signal (that, in the last softmax layer of our network would mean a number close to 1). \n",
    "\n",
    "* TP (true positive): the event is signal, the prediction is signal (*correct result*)\n",
    "* FP (false positive): the event is background, but the prediction is signal (*unexpected result*)\n",
    "* TN (true negative): the event is background, the prediction is background (*correct absence of signal*)\n",
    "* FN (false negative): the event is signal, the prediction is background (*missing a true signal event*)\n",
    "\n",
    "Some additional definitions:\n",
    "\n",
    "* **TPR (true positive rate)**: how often the network predicts a positive outcome (*signal*), when the input is positive (*signal*): $TPR = \\frac{TP}{TP+FN}$\n",
    "* **FPR (false positive rate)**: how often the network predicts a positive outcome (*signal*), when the input is negative (*background*): $FPR = \\frac{FP}{FP+TN}$\n",
    "\n",
    "A good classifier should give an high TPR and a small FPR.\n",
    "\n",
    "Quoting wikipedia:\n",
    "\n",
    "\"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, probability of detection, or signal efficiency in high energy physics. The false-positive rate is also known as the probability of false alarm or fake rate in high energy physics.\"\n",
    "\n",
    "\n",
    "The ROC curve requires the true binary value (0 or 1, background or signal) and the probability estimates of the positive (signal) class.\n",
    "\n",
    "This figure shows an example of such a ROC curve, than can be obtained with **`roc_curve`** method available in sklearn libraries:\n",
    "<br>\n",
    "<img src=\"https://github.com/laramaktub/imagenes-hep/raw/master/roc.png\" width=\"500\" >\n",
    "\n",
    "The **`roc_auc_score`** function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve.\n",
    "\n",
    "\n",
    "The AUC is the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. The higher the AUC, the better the performance of the classifier. If the AUC is 0.5, the classifier is uninformative, i.e., it will rank equally a positive or a negative observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WY3wXgY-Et-J"
   },
   "source": [
    "Now we can build the ROC curve. Let's try to understand what is done each step.\n",
    "\n",
    "First we need to predict the probabilities of each event in our test sample. We can use the function **`predict`** (or **`predict_proba`** that is equivalent). Let's print out its output.\n",
    "\n",
    "The number of computed probabilities for each row corresponds the number of the classification categories (2 in our case, hence we get two columns).\n",
    "\n",
    "* The first column tells us: *what is the probability that this event is classified in the class 0, i.e., as background?*\n",
    "* The second column tells us: *what is the probability that this event is classified in the class 1, i.e., as signal?*\n",
    "\n",
    "We want to compute the TPR and the FPR, hence we are interested in the second column (we are asking ourself, *how likely it is that this event is signal?*). Then we save this output as a column of the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL_1fpd7Et-K"
   },
   "source": [
    "Let's calculate first the AUC of our previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kh7gf-kEt-K"
   },
   "outputs": [],
   "source": [
    "# Needed libraries\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NAPwEhsEt-N"
   },
   "outputs": [],
   "source": [
    "print(\"Running on test sample. This may take a moment.\")\n",
    "probs = model.predict(df_test[cols])#predict probability over test sample\n",
    "AUC = roc_auc_score(df_test[\"is_signal_new\"], probs[:,1])\n",
    "print(\"Test Area under Curve = {0}\".format(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rind16ANEt-R"
   },
   "outputs": [],
   "source": [
    "print(probs)\n",
    "df_test[\"sigprob\"] = probs[:,1] #save probabilities in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ccihnz5Et-V"
   },
   "source": [
    "It is very instructive to get a feeling of what are representing these probabilities. Let's save them as two separate numpy arrays, for background and signal. Let's plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjFza4ALEt-W"
   },
   "outputs": [],
   "source": [
    "back = np.array(df_test[\"sigprob\"].loc[df_test[\"is_signal_new\"]==0].values)\n",
    "sign = np.array(df_test[\"sigprob\"].loc[df_test[\"is_signal_new\"]==1].values)\n",
    "#saves the df_test[\"sigprob\"] column when the event is signal or background\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "\n",
    "#Let's plot an histogram:\n",
    "# * y-values: back/sign probabilities\n",
    "# * 50 bins\n",
    "# * alpha: filling color transparency\n",
    "# * density: it should normalize the histograms to unity\n",
    "plt.hist(back, 50, color='blue', edgecolor='blue', lw=2, label='background', alpha=0.3, density=True)\n",
    "plt.hist(sign, 50, color='red', edgecolor='red', lw=2, label='signal', alpha=0.3, density=True)\n",
    "\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.xlabel('Event probability of being classified as signal')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-Ytqy5iEt-Z"
   },
   "source": [
    "We can clearly appreciate that background events (in blue) have a lower probability of being classified as signal, whilst signal events (in red) have a larger probability of being classified as signal.\n",
    "\n",
    "Then we use the **`roc_curve`** method and we retain the outputs (that are, the TPR and the FPR). TPR and FPR are obtained by scanning the values of the previous histogram. We can finally plot the curve and the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4swpPyU4Et-a"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(df_test[\"is_signal_new\"], df_test[\"sigprob\"]) #extract true positive rate and false positive rate\n",
    "plt.figure(figsize=(8,7))\n",
    "plt.rcParams.update({'font.size': 15}) #Larger font size\n",
    "plt.plot(fpr, tpr, color='crimson', lw=2, label='ROC curve (area = {0:.4f})'.format(AUC))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yox2N8TXEt-e"
   },
   "source": [
    "These performances are not that great. Can they be improved?\n",
    "\n",
    "* We are just using the first 20 jet constituents, try to add more constituents! And remember to change the **`input_shape`** accordingly.\n",
    "* Add more data to the training/validation/testing sample (but don't use the same data for testing!). A good rule is: if you use $n_T$ events for training, use $0.2 \\times n_T$ for both validation and testing.\n",
    "* Try to add more epochs (beware of overfitting!).\n",
    "* Try to add more **`Dense`** layers, with a different number of nodes.\n",
    "* You can also try a different optimizer, different loss function...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pz6XG90MEt-f"
   },
   "source": [
    "# Challenge submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffVwJdnEEt-h"
   },
   "source": [
    "Once you manage to improve the network performances, you can submit your results and participate to our ML challenge. The challenge test sample is available in this workspace, but the truth labels (`is_signal_new`) are removed, so that you can't compute the AUC.\n",
    "\n",
    "* You can participate as a single participant or as a team\n",
    "* The winner is the one scoring the best AUC in the challenge test sample\n",
    "* In the next box, you will find some lines of code for preparing an output zip file, containing your model and the weights you obtained out of your training\n",
    "* Choose a meaningful name for your result zip file (i.e. your name, or your team name, but avoid to submit `results.zip`)\n",
    "* Download the zip file and send it to me\n",
    "* You can submit multiple results, paying attention to name them accordingly (add the version number, such as `v1`, `v34`, etc.)\n",
    "* You can use both TopTagging_1 or TopTagging_2 as a starting point (train over constituents or over images)\n",
    "* We will consider your best result for the final score\n",
    "\n",
    "\n",
    "**Have fun!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lu9Bbvx9Et-i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Evaluate performance on independent sample\n",
    "# DO NOT CHANGE BELOW!\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "# Prepare input\n",
    "store_test_challenge = pd.HDFStore(\"/home/jovyan/share/TopTaggingData/test_without_truth_100k.h5\")\n",
    "df_test_challenge = store_test_challenge.select(\"table\")\n",
    "\n",
    "# Run DNN\n",
    "print(\"Running on full test sample. This may take a moment.\")\n",
    "ret = model.predict(df_test_challenge[cols])\n",
    "np.save(\"result.npy\",ret[:,1])\n",
    "\n",
    "#Save in zip:\n",
    "#!zip result.zip result.npy\n",
    "import zipfile\n",
    "zipf = zipfile.ZipFile('result.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipf.write('result.npy')\n",
    "zipf.close()\n",
    "print(\"Done. Click below  to download result\")\n",
    "FileLink('result.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ejercicio_TopTagging_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
